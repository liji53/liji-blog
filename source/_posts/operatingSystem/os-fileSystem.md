
---
title: os-文件系统
date: 2021-07-10 20:11:07
tags:
categories: 操作系统
---

- [文件系统](#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F)
    - [HDD和SSD基础](#HDD%E5%92%8CSSD%E5%9F%BA%E7%A1%80)
        - [HDD的硬件特性](#HDD%E7%9A%84%E7%A1%AC%E4%BB%B6%E7%89%B9%E6%80%A7)
        - [SSD的硬件特性](#SSD%E7%9A%84%E7%A1%AC%E4%BB%B6%E7%89%B9%E6%80%A7)
    - [文件系统的实现](#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E7%8E%B0)
        - [1.Ext2的布局(物理结构)](#1-Ext2%E7%9A%84%E5%B8%83%E5%B1%80-%E7%89%A9%E7%90%86%E7%BB%93%E6%9E%84)
        - [2.Ext2目录结构](#2-Ext2%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84)
        - [3.Fat的布局](#3-Fat%E7%9A%84%E5%B8%83%E5%B1%80)
        - [4.Ext4的数据管理方式](#4-Ext4%E7%9A%84%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E6%96%B9%E5%BC%8F)
    - [文件读写的过程](#%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%E7%9A%84%E8%BF%87%E7%A8%8B)
        - [1.读IO时序](#1-%E8%AF%BBIO%E6%97%B6%E5%BA%8F)
        - [2.写IO时序](#2-%E5%86%99IO%E6%97%B6%E5%BA%8F)
        - [3.性能优化](#3-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96)
    - [异常恢复-文件系统一致性](#%E5%BC%82%E5%B8%B8%E6%81%A2%E5%A4%8D-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%80%E8%87%B4%E6%80%A7)
        - [1.fsck](#1-fsck)
        - [2.通过日志恢复](#2-%E9%80%9A%E8%BF%87%E6%97%A5%E5%BF%97%E6%81%A2%E5%A4%8D)
    - [命令与资料](#%E5%91%BD%E4%BB%A4%E4%B8%8E%E8%B5%84%E6%96%99)
    
# 文件系统
文件系统是OS用来组织和分配存储设备的方法，现在linux系统用的基本都是Ext4文件系统，在window上则基本都是NTFS文件系统，接下来我们将学习它们的物理结构以及实现原理。

### HDD和SSD基础
在了解文件系统的实现之前，需要先知道存储介质的特性，因为文件系统(包括很多应用程序，像数据库)正是基于这些特征才设计的。
##### HDD的硬件特性
![](Images/HDD.png)
如上图，你需要清楚的是
1. 扇区：即图中的编号，操作硬盘的基本存储单元，**对扇区的读写是原子的**
2. 寻道：图中seek的动作，寻道时间一般在**几毫秒到十几毫秒**
3. 旋转：图中Rotates的动作，旋转一圈的时间一般也要**几毫秒**(如10000RPM = 6 ms/R)
4. 以现有的技术发展，寻道、旋转时间的进步比内存、cpu的发展慢的多，甚至是停滞的

一次读写实际花费的时间应该是：
T(I/O) = Tseek + Trotation + Ttransfer
其中Ttransfer一般是100M/s到几百M/s之间
有了这个公式之后，在对文件系统的设计(包括应用程序的设计)，要尽可能的利用这个公式，提高传输效率。
在涉及读写的时候，主要考虑二种读写模式：
1. 随机读写：**在随机读写中，每次磁头都需要重新定位**，它的IO时间为Tseek + Trotation + Ttransfer
2. 顺序读写：**在顺序读写中**，如果一次性写入一大块的数据，**则只需要一次的Tseek和Trotation 即可**。但如果是分批次的顺序写入，除了第一次的Tseek和Trotation时间，后面每次读写还需要Trotation时间(因为磁盘是一直在转的，分批次写入会有时间差，导致磁盘转过头)

##### SSD的硬件特性
关于SSD，你需要清楚的是：
1. Page: 读写以page为单位，一般4k或8K
2. Block：删除的基本单位，由多个page组成
3. SSD内部一般会维护一个mapping table，维护逻辑地址到物理地址的映射，不需要寻道和旋转，可以直接计算出物理地址。它的定位时间一般在0.1ms
4. SSD如果损坏了，数据很难恢复，而HDD要容易的多
5. HDD没有写入次数的限制，SSD存在写入寿命的限制（因此有损耗均衡的机制）

基于这些硬件特性，SSD在随机读写上的性能要远好于HDD，而在顺序读写上一般也比HDD快1倍。（SSD内部的机制如垃圾回收机制，损耗均衡等不再介绍）

### 文件系统的实现
文件系统会把硬盘分割成一个个块，以块为单位进行读写，一般一个块4K大小。每个块只属于一个文件(目录)，如果文件不足block大小，则剩余的容量不再使用

##### 1.Ext2的布局(物理结构)
Ext2文件系统的布局(图片来源：https://blog.csdn.net/gongjiwei/article/details/82025142):
![](Images/Ext2_layout.png)
下面对上图中的术语进行解释：
MBR: 引导块，常用于启动电脑
block group：由诺干个块组成的，为了提高访问同一目录下文件的效率(减少Tseek)，文件系统会把同一个目录下的文件尽量放在一个group中。
superblock：记录此文件系统的整体信息，如块大小，容量等，每个block group都有，用于备份。
group description table: 块组描述符表，记录了每个块组的块位图、inode位图、inode table等的位置，同样每个block group都有，用于备份。
block bitmap：块位图中的每一位表示当前块组中对应的那个块是否被使用。自身占一个块
inode bitmap：i节点位图中的每一位表示inode是否可用。自身占一个块
data blocks：数据块，用于存放文件、目录的数据
inode table:ext2中一个inode占用128byte（记录文件的属性，不包括文件名）,它的属性如下：
![](Images/Ext2_inode.png)
这里可以看到inode中存储了数据块的地址(block字段)，再结合文件系统布局图，可以看到一共有15个地址。
但为什么要设计成12个direct block以及3个一级、二级、三级间接block呢？（这么设计对寻址有什么好处？）
1. 设计成**多级间接block是为了支持大容量的文件**：(12 + 1024 + 1024\^2 + 1024\^3) × 4 KB
2. 设计成**12个direct block，是因为大多数文件的大小都小于2k，这样就可以直接定位到block**，而不用去data block间接查找，提高效率

##### 2.Ext2目录结构
data blocks既可以用于存放文件的数据，也可以用于存放目录的数据.
每一个目录项的数据结构如下：
```c
　struct ext2_dir_entry_2 {
　　__le32 inode; // 文件入口的inode号，0表示该项未使用
　　__le16 rec_len; // 目录项长度
　　__u8 name_len; // 文件名包含的字符数
　　__u8 file_type; // 文件类型
　　char name[255]; // 文件名
　　};
```
目录会保存此目录下的所有文件名和目录名，以及文件的入口地址。至于**其他的信息则存放在inode节点中(注意：fat文件系统放在目录项中)**

##### 3.Fat的布局
了解完基于inode组织的文件系统之后，我们再来了解下基于链表组织的Fat文件系统。像SD卡的文件系统一般就是Fat32，在Fat文件系统中对应Ext2 Block的描述是簇。
它的磁盘布局如下：
![](Images/fat_layout.png)
其中Fat表的位置由DBR来定位，Fat2是Fat1的备份，而Fat表的数据会读到内存中，它的布局如下：
![](Images/fat_table.png)
它描述了：
1. 簇的分配情况(0表示没有分配)，因此不存在像bitmap这样的数据结构
2. 文件下一个数据块的簇号(-1表示结束)，因此不需要类似inode的block字段

但从上面的布局中，我们没有找到用于描述文件信息的元数据，其实这些**元信息都存放在目录项中**，而目录项的数据又存放在数据块中，因此这里看不到。
如果想要实际看看文件系统的结构，可以用WinHex可查看存储介质的二进制数据

##### 4.Ext4的数据管理方式
Ext2、Ext3基于表的方式查找数据块;Fat基于链表的方式查找数据块;而Ext4基于extent tree的方式查找数据块。
Ext4的数据布局可以直接参考Ext2的布局，下面我们主要看Ext4是如何查找数据块的。
Ext4的inode 同样存在i_block[15] 结构，除了可以用于新的Extent tree结构，还可以兼容老的Ext系统。
先看Ext2采用表来查找数据块的问题：如果一个文件分配了连续的1000个块，却需要映射这1000个块的地址。
而采用extent的方案，**连续地址的映射只需要一个起始地址+长度**（ee_len）即可。
extent tree的访问过程：（图片来源：https://zhuanlan.zhihu.com/p/52052278）
![](Images/Ext4_extent.jpg)
从数据结构来看，虽然它的名字叫extent tree，但其实与B-树类似。
关于Ext4的详细数据结构信息，可以看最后一节给出的链接地址，这里不再详细介绍。

### 文件读写的过程
看完上面常见文件系统的布局之后，我们还需要清楚open、read等函数背后的IO动作
##### 1.读IO时序
假设我们的代码如下：
```c
int fd = open("/foo/bar", O_RDONLY);
size = read(fd, buffer, sizeof(buffer));
```
它的IO过程如下：
![](Images/read_access.png)
假如没有采用任何手段，一句open就要产生5次IO访问，在结合HDD的硬件特性，由于这些block是分散的，因此5次IO意味着5*(Tseek+Trotation),基本上需要50ms以上。

##### 2.写IO时序
类似的，写的过程如下：
![](Images/write_access.png)

##### 3.性能优化
对于读的性能优化，最常用的方案自然是缓存，通过把常用的inode节点缓存起来，就可以大大的减少访问次数。
如前面读IO时序中的图，如果把bar inode节点缓存起来，就可以减少后面3次的读写，而常见的缓存策略自然是LRU。
写的性能优化，可以通过缓冲buffer来实现，一般的缓冲的设计从时间与空间角度出发，比方每5s写入磁盘一次，或者buffer满了就写一次磁盘。
但buffer应该设计成多大呢？
这里我们假设一次Tseek+Trotation的时间是10ms，传输速率为40MB/s
如果我们想要达到50%的传输宽带，buffer的大小应该是 40\*1024\*10/1000 = 409 KB
虽然使用缓冲虽然能提高传输效率，但如果突然宕机了，就存在数据丢失的风险。

### 异常恢复-文件系统一致性
文件系统在写入数据时，需要修改多个块数据(bitmap、inode、数据块等)，但这个过程并不是原子的。因此如果在写入的过程中突然宕机，就会产生文件系统不一致的情况，这时候就需要进行修复。
这里我们仅讨论对元数据的修复，而像数据块的丢失、不一致的问题可以在数据库事务中再讨论。

##### 1.fsck
这是在linux系统中自带的工具，它的原理就是扫描bitmap表和inode table，把它们分别记录在2张表中，如下图A:
![](Images/fsck.png)
图A表示数据一致，不需要修复。
图B表示实际数据块已经分配，但bitmap表无记录，因此只需要把bitmap表的对应位置0(已用)即可。
图D的情况比较复杂，可能是某一个文件X刚删除了一个数据块，同时把bitmap表置1(空闲)，但紧接着另一个文件Y申请到了相同的块，于是又把bitmap置0，但还没来的及修改A的inode就宕机了。这种情况fsck也无法修复，因为不知道这个数据块到底属于谁（这时候可以咨询用户，由用户自己决定）。
也有一些问题无法修复，比如inode指向错误的数据块，fsck仅能确保元数据的一致性

##### 2.通过日志恢复
通过fsck修复文件系统存在一个问题：太慢了。（之前我做的一个项目就是在SD卡挂载之前调用fsck来恢复文件系统的，导致用户插入SD卡的体验非常差，可能需要等20s才能看到SD卡挂载成功）
下面我们介绍ext3、ext4(ext2不支持)文件系统所使用的方法，**Write Ahead Log策略：每次更新将数据记录到日志文件中，然后在修改实际数据，如果宕机了，通过日志文件对数据进行恢复。**
日志的数据格式如下：
![](Images/ext3_journal.png)
TxB和TxE表示这条日志的开始和结束，它记录了这条日志的ID。
按照上图的格式，如果是先一次性写日志，在写实际数据，这样存在3个问题：
1. 写日志的操作不是原子的，一次性写这条数据，可能出现头和尾都写了，但中间数据缺失，但恢复的时候由于头尾已经存在就认为数据完整，但其实是不完整的。
2. 不停的写日志，日志满了怎么办。
3. 数据要写2次，对磁盘空间的利用率以及性能都有影响。

为了解决第一个问题，我们要对日志的写入(日志的写入也采用缓冲)顺序做调整：
1. 先写TxB + 日志数据
2. 等到步骤1完成，在写入TxE (思路类似于多线程中的内存 barrier)
3. 最后再写入实际的元数据与数据块，这个过程也叫checkpoint

基于这个写入顺序，在恢复的时候，只要看TxB和TxE是否匹配就能知道日志中的数据是否完整，这个过程其实也就是ext3、ext4的journal模式。

第二个问题则可以通过采用环形队列的数据结构来解决:
![](Images/journal_circle.png)
在journal的头部增加一个超级块，在超级块中标记最新的日志和最老的日志（理解成2个指针）,在数据满的情况下，每次新写入的日志都覆盖最老的日志。

最后一个问题我们可以让实际数据的写入最先执行来实现，因为我们的目标是保证元数据的一致性。
如果写入了数据块，但元数据没写入，最多也只是丢失本次的数据而已，而不会破坏文件系统的结构。
因此，日志写入顺序的最终版本如下：
1. 先写实际的数据，即文件内容
2. 写日志中的TxB + 元数据
3. **必须等到步骤2完成，再写入TxE**
4. 更新元数据（实际数据的写入已经在第一步完成）
5. 等步骤4完成之后，更新日志超级块的2个指针（最新和最旧）

用图表示：
![](Images/journal_timeline.png)
这个过程就是ext3、ext4的ordered模式（默认的模式，可能还存在一些细微差别）

### 命令与资料
命令：
    查看是HDD还是SSD的命令：lsblk -d -o name,rota  （1表示HDD，0表示SSD）
    查看block size：tune2fs -l /dev/sda1 | grep "Block size"
书籍：
《Operating Systems: Three Easy Pieces》([线上书籍](https://pages.cs.wisc.edu/~remzi/OSTEP/))
《Modern Operating Systems》（第四版）

HDD/SSD基础知识及工作原理：https://blog.csdn.net/qq_23929673/article/details/103429583
SSD程序员需要知道的SSD基本原理：https://zhuanlan.zhihu.com/p/104995703
Ext4的详细信息：https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout
